{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "from lxml import etree\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\harsh\\AppData\\Local\\Temp\\tmp03l6a1w7\\config.json as plain json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 1488\n"
     ]
    }
   ],
   "source": [
    "file_path = \"data/semeval14/Laptop_Train_v2_text.txt\"\n",
    "model_path = \"biaffine-dependency-parser-ptb-2020.04.06.tar.gz\"\n",
    "\n",
    "predictor = Predictor.from_path(model_path)\n",
    "\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    sentences = f.readlines()\n",
    "\n",
    "print(\"Number of sentences: {}\".format(len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': 'This is the current generation, looking for jobs all the time', 'tokens': ['This', 'is', 'the', 'current', 'generation', ',', 'looking', 'for', 'jobs', 'all', 'the', 'time'], 'tags': ['DET', 'AUX', 'DET', 'ADJ', 'NOUN', 'PUNCT', 'VERB', 'ADP', 'NOUN', 'DET', 'DET', 'NOUN'], 'predicted_dependencies': ['nsubj', 'cop', 'dep', 'dep', 'root', 'punct', 'amod', 'prep', 'pobj', 'dep', 'amod', 'dep'], 'predicted_heads': [5, 5, 5, 5, 0, 5, 5, 7, 8, 9, 12, 9], 'dependencies': [['nsubj', 5, 1], ['cop', 5, 2], ['dep', 5, 3], ['dep', 5, 4], ['root', 0, 5], ['punct', 5, 6], ['amod', 5, 7], ['prep', 7, 8], ['pobj', 8, 9], ['dep', 9, 10], ['amod', 12, 11], ['dep', 9, 12]], 'aspect_sentiment': ['generation', 'jobs', 'time'], 'from_to': [(4, 0), (8, 8), (11, 9)]}\n"
     ]
    }
   ],
   "source": [
    "def parsing_pipeline(given_sentence):\n",
    "    sentence_dict = dict()\n",
    "    parse_predict = predictor.predict(sentence=given_sentence)\n",
    "\n",
    "    sentence_dict[\"sentence\"] = given_sentence\n",
    "    sentence_dict['tokens'] = parse_predict['words']\n",
    "    sentence_dict['tags'] = parse_predict['pos']\n",
    "    \n",
    "    predicted_dependencies = parse_predict['predicted_dependencies']\n",
    "    predicted_heads = parse_predict['predicted_heads']\n",
    "    \n",
    "    sentence_dict['predicted_dependencies'] = parse_predict['predicted_dependencies']\n",
    "    sentence_dict['predicted_heads'] = parse_predict['predicted_heads']\n",
    "    sentence_dict['dependencies'] = []\n",
    "    \n",
    "    for idx, item in enumerate(predicted_dependencies):\n",
    "        dep_tag = item\n",
    "        frm = predicted_heads[idx]\n",
    "        to = idx + 1\n",
    "        sentence_dict['dependencies'].append([dep_tag, frm, to])\n",
    "        \n",
    "    sentence_dict[\"aspect_sentiment\"] = []\n",
    "    sentence_dict['from_to'] = [] #left and right offset of the target word \n",
    "\n",
    "    for index_sentence, tag in enumerate(sentence_dict[\"tags\"]):\n",
    "        if tag == \"NOUN\" or tag==\"PROPN\":\n",
    "            sentence_dict[\"aspect_sentiment\"].append((sentence_dict['tokens'][index_sentence]))\n",
    "            from_index = index_sentence\n",
    "            if from_index != 0:\n",
    "                to = sentence_dict['predicted_heads'][from_index - 1]\n",
    "                if sentence_dict['predicted_heads'][from_index] == to and sentence_dict['predicted_heads'][from_index + 1] !=to:\n",
    "                    to_index = sentence_dict['predicted_heads'][from_index + 1]\n",
    "                else:\n",
    "                    to_index = sentence_dict['predicted_heads'][from_index]\n",
    "\n",
    "\n",
    "            sentence_dict['from_to'].append((from_index, to_index))\n",
    "    \n",
    "    return sentence_dict\n",
    "\n",
    "\n",
    "given_sentence = \"This bag is not mine, mines and dangerous.\"\n",
    "\n",
    "print(parsing_pipeline(given_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
