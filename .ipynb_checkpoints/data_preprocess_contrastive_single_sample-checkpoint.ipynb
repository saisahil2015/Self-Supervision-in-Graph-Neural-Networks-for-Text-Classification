{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synonym and Antonym pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from transformers import HfArgumentParser, PreTrainedTokenizer\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from datasets import load_dataset, Dataset\n",
    "import os\n",
    "from spacy.tokens import Doc\n",
    "from random_words import RandomWords\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from lemminflect import getInflection\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "from lxml import etree\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "random.seed(12345)\n",
    "\n",
    "Doc.set_extension('_synonym_sent', default=False)\n",
    "Doc.set_extension('_synonym_intv', default=False)\n",
    "Doc.set_extension('_ori_syn_intv', default=False)\n",
    "Doc.set_extension('_antonym_sent', default=False)\n",
    "Doc.set_extension('_antonym_intv', default=False)\n",
    "Doc.set_extension('_ori_ant_intv', default=False)\n",
    "\n",
    "file_path = \"data/semeval14/Laptop_Train_v2_text.txt\"\n",
    "model_path = \"biaffine-dependency-parser-ptb-2020.04.06.tar.gz\"\n",
    "\n",
    "predictor = Predictor.from_path(model_path)\n",
    "\n",
    "\n",
    "rw = RandomWords()\n",
    "\n",
    "REPLACE_RATIO = 0.3\n",
    "\n",
    "REPLACE_ORIGINAL = 0\n",
    "REPLACE_LEMMINFLECT = 1\n",
    "REPLACE_SYNONYM = 2\n",
    "REPLACE_HYPERNYMS = 3\n",
    "REPLACE_ANTONYM = 4\n",
    "REPLACE_RANDOM = 5\n",
    "REPLACE_ADJACENCY = 6\n",
    "\n",
    "REPLACE_NONE = -100\n",
    "\n",
    "SYNONYM_RATIO = 1/3\n",
    "HYPERNYMS_RATIO = 1/3\n",
    "LEMMINFLECT_RATIO = 1/3\n",
    "\n",
    "ANTONYM_RATIO = 1/2\n",
    "RANDOM_RATIO = 1/2\n",
    "\n",
    "REPLACE_TAG = ['NN', 'NNS', 'JJ', 'JJR', 'JJS', 'RB', 'RBR',\n",
    "               'RBS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']  # [NNP, NNPS]\n",
    "REPLACE_POS = ['NOUN', 'VERB', 'ADJ', 'ADV']\n",
    "POS_TO_TAGS = {'NOUN': ['NN', 'NNS'],\n",
    "               'ADJ': ['JJ', 'JJR', 'JJS'],\n",
    "               'VERB': ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'],\n",
    "               'ADV': ['RB', 'RBR', 'RBS']}\n",
    "\n",
    "\n",
    "def get_synonym(token):\n",
    "    lemma = token.lemma_\n",
    "    text = token.text\n",
    "    tag = token.tag_\n",
    "    pos = token.pos_\n",
    "    word_synset = set()\n",
    "    if pos not in REPLACE_POS:\n",
    "        return list(word_synset)\n",
    "\n",
    "    synsets = wn.synsets(text, pos=eval(\"wn.\"+pos))\n",
    "    for synset in synsets:\n",
    "        words = synset.lemma_names()\n",
    "        for word in words:\n",
    "            # word = wnl.lemmatize(word, pos=eval(\"wn.\"+pos))\n",
    "            if word.lower() != text.lower() and word.lower() != lemma.lower():\n",
    "                # inflt = getInflection(word, tag=tag)\n",
    "                # word = inflt[0] if len(inflt) else word\n",
    "                word = word.replace('_', ' ')\n",
    "                word_synset.add(word)\n",
    "\n",
    "    return list(word_synset)\n",
    "\n",
    "\n",
    "def get_hypernyms(token):\n",
    "    lemma = token.lemma_\n",
    "    text = token.text\n",
    "    tag = token.tag_\n",
    "    pos = token.pos_\n",
    "    word_hypernyms = set()\n",
    "    if pos not in REPLACE_POS:\n",
    "        return list(word_hypernyms)\n",
    "\n",
    "    synsets = wn.synsets(text, pos=eval(\"wn.\"+pos))\n",
    "    for synset in synsets:\n",
    "        for hyperset in synset.hypernyms():\n",
    "            words = hyperset.lemma_names()\n",
    "            for word in words:\n",
    "                # word = wnl.lemmatize(word, pos=eval(\"wn.\"+pos))\n",
    "                if word.lower() != text.lower() and word.lower() != lemma.lower():\n",
    "                    # inflt = getInflection(word, tag=tag)\n",
    "                    # word = inflt[0] if len(inflt) else word\n",
    "                    word = word.replace('_', ' ')\n",
    "                    word_hypernyms.add(word)\n",
    "\n",
    "    return list(word_hypernyms)\n",
    "\n",
    "\n",
    "def get_antonym(token):\n",
    "    lemma = token.lemma_\n",
    "    text = token.text\n",
    "    tag = token.tag_\n",
    "    pos = token.pos_\n",
    "    word_antonym = set()\n",
    "    if pos not in REPLACE_POS:\n",
    "        return list(word_antonym)\n",
    "\n",
    "    synsets = wn.synsets(text, pos=eval(\"wn.\"+pos))\n",
    "    for synset in synsets:\n",
    "        for synlemma in synset.lemmas():\n",
    "            for antonym in synlemma.antonyms():\n",
    "                word = antonym.name()\n",
    "                # word = wnl.lemmatize(word, pos=eval(\"wn.\"+pos))\n",
    "                if word.lower() != text.lower() and word.lower() != lemma.lower():\n",
    "                    # inflt = getInflection(word, tag=tag)\n",
    "                    # word = inflt[0] if len(inflt) else word\n",
    "                    word = word.replace('_', ' ')\n",
    "                    word_antonym.add(word)\n",
    "\n",
    "    return list(word_antonym)\n",
    "\n",
    "\n",
    "def get_lemminflect(token):\n",
    "    text = token.text\n",
    "    lemma = token.lemma_\n",
    "    tag = token.tag_\n",
    "    pos = token.pos_\n",
    "    word_lemminflect = set()\n",
    "    if pos not in REPLACE_POS:\n",
    "        return list(word_lemminflect)\n",
    "\n",
    "    tags = POS_TO_TAGS[pos]\n",
    "    for tg in tags:\n",
    "        if tg == tag:\n",
    "            continue\n",
    "        inflects = getInflection(lemma, tag=tg)\n",
    "        for word in inflects:\n",
    "            if word.lower() != text.lower():\n",
    "                word_lemminflect.add(word)\n",
    "\n",
    "    return list(word_lemminflect)\n",
    "\n",
    "\n",
    "def search_replacement(doc, candidate_index, replace_type, max_num, pos_to_words=None):\n",
    "    sr_rep = []\n",
    "    if max_num < 1:\n",
    "        return sr_rep\n",
    "\n",
    "    for r_idx in candidate_index:\n",
    "        token = doc[r_idx]\n",
    "        rep = None\n",
    "        if replace_type == REPLACE_ANTONYM:\n",
    "            reps = get_antonym(token)\n",
    "            rep = random.choice(reps) if reps else None\n",
    "        elif replace_type == REPLACE_ADJACENCY:\n",
    "            reps = pos_to_words[token.pos_]\n",
    "            rep = random.choice(reps) if reps else None\n",
    "        elif replace_type == REPLACE_RANDOM:\n",
    "            rep = rw.random_word()\n",
    "        elif replace_type == REPLACE_SYNONYM:\n",
    "            reps = get_synonym(token)\n",
    "            rep = random.choice(reps) if reps else None\n",
    "        elif replace_type == REPLACE_HYPERNYMS:\n",
    "            reps = get_hypernyms(token)\n",
    "            rep = random.choice(reps) if reps else None\n",
    "        elif replace_type == REPLACE_LEMMINFLECT:\n",
    "            reps = get_lemminflect(token)\n",
    "            rep = random.choice(reps) if reps else None\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        if rep and rep.lower() != token.text.lower():\n",
    "            sr_rep.append((r_idx, rep, replace_type))\n",
    "\n",
    "        if len(sr_rep) >= max_num:\n",
    "            break\n",
    "\n",
    "    # print(\"Search Replacement: \\n\", sr_rep)\n",
    "    return sr_rep\n",
    "\n",
    "\n",
    "def replace_word(doc, pairs):\n",
    "    synonym_sent = []\n",
    "    synonym_intv = []\n",
    "    ori_syn_intv = []\n",
    "    antonym_sent = []\n",
    "    antonym_intv = []\n",
    "    ori_ant_intv = []\n",
    "\n",
    "    length = len(doc)\n",
    "    rep_num = int(length*REPLACE_RATIO)\n",
    "\n",
    "    rep_index = []\n",
    "    # pos_word = {p:[] for p in REPLACE_POS}\n",
    "    for index, token in enumerate(doc):\n",
    "        if token.pos_ in REPLACE_POS:\n",
    "            rep_index.append(index)\n",
    "            # pos_word[token.pos_].append(token.text)\n",
    "\n",
    "    rep_num = min(rep_num, len(rep_index))\n",
    "\n",
    "    syn_rand = random.random()\n",
    "    ant_rand = random.random()\n",
    "\n",
    "    syn_index = rep_index[:]\n",
    "    random.shuffle(syn_index)\n",
    "    ant_index = rep_index[:]\n",
    "    random.shuffle(ant_index)\n",
    "\n",
    "    syn_replace = []\n",
    "    ant_replace = []  # [(rep_idx, rep_word, rep_type)]\n",
    "\n",
    "    ############### Antonym Replacement ####################\n",
    "    if ant_rand < ANTONYM_RATIO:\n",
    "        ant_replace = search_replacement(\n",
    "            doc, candidate_index=ant_index, replace_type=REPLACE_ANTONYM, max_num=rep_num)\n",
    "        # print(\"Ant_replace1: \\n\", ant_replace)\n",
    "\n",
    "    # if not ant_replace and ant_rand < ANTONYM_RATIO + ADJACENCY_RATIO:\n",
    "    #     ant_replace = search_replacement(doc, candidate_index=ant_index, replace_type=REPLACE_ADJACENCY, max_num=rep_num, pos_to_words=pos_word)\n",
    "\n",
    "    if not ant_replace:\n",
    "        ant_replace = search_replacement(\n",
    "            doc, candidate_index=ant_index, replace_type=REPLACE_RANDOM, max_num=rep_num)\n",
    "        # print(\"Ant_replace2: \\n\", ant_replace)\n",
    "\n",
    "    ############### Synonym Replacement ####################\n",
    "    if syn_rand < HYPERNYMS_RATIO:\n",
    "        syn_replace = search_replacement(\n",
    "            doc, candidate_index=syn_index, replace_type=REPLACE_HYPERNYMS, max_num=rep_num)\n",
    "        # print(\"syn_replace1: \\n\", syn_replace)\n",
    "\n",
    "    if not syn_replace and syn_rand < HYPERNYMS_RATIO + SYNONYM_RATIO:\n",
    "        syn_replace = search_replacement(\n",
    "            doc, candidate_index=syn_index, replace_type=REPLACE_SYNONYM, max_num=rep_num)\n",
    "        # print(\"syn_replace2: \\n\", syn_replace)\n",
    "\n",
    "    if not syn_replace:\n",
    "        syn_replace = search_replacement(\n",
    "            doc, candidate_index=syn_index, replace_type=REPLACE_LEMMINFLECT, max_num=rep_num)\n",
    "        # print(\"syn_replace3:\\n \", syn_replace)\n",
    "    ############### Original Replacement ####################\n",
    "\n",
    "    all_replace = ant_replace + syn_replace\n",
    "    all_replace = sorted(all_replace, key=lambda x: x[0], reverse=True)\n",
    "    # print(\"All Replace: \\n\", all_replace)\n",
    "\n",
    "    ori_len = -1  # point to the space before next token\n",
    "    syn_len = -1\n",
    "    ant_len = -1\n",
    "    rep_idx, rep_word, rep_type = all_replace.pop(\n",
    "    ) if all_replace else (None, None, None)\n",
    "    for index, token in enumerate(doc):\n",
    "        ori = syn = ant = token.text\n",
    "\n",
    "        while index == rep_idx:\n",
    "            if rep_type in [REPLACE_SYNONYM, REPLACE_HYPERNYMS, REPLACE_LEMMINFLECT]:\n",
    "                syn = rep_word\n",
    "                # fix length mismatch, mx.encode for bytelevelbpe\n",
    "                synonym_intv.append(\n",
    "                    (syn_len, syn_len + len(syn.encode('utf-8')), rep_type))\n",
    "                ori_syn_intv.append(\n",
    "                    (ori_len, ori_len + len(ori.encode('utf-8')), rep_type))\n",
    "            elif rep_type in [REPLACE_ANTONYM, REPLACE_RANDOM]:\n",
    "                ant = rep_word\n",
    "                antonym_intv.append(\n",
    "                    (ant_len, ant_len + len(ant.encode('utf-8')), rep_type))\n",
    "                ori_ant_intv.append(\n",
    "                    (ori_len, ori_len + len(ori.encode('utf-8')), rep_type))\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            rep_idx, rep_word, rep_type = all_replace.pop(\n",
    "            ) if all_replace else (None, None, None)\n",
    "\n",
    "        if index in rep_index:\n",
    "            if ori == syn:\n",
    "                synonym_intv.append(\n",
    "                    (syn_len, syn_len + len(syn.encode('utf-8')), REPLACE_ORIGINAL))\n",
    "                ori_syn_intv.append(\n",
    "                    (ori_len, ori_len + len(ori.encode('utf-8')), REPLACE_ORIGINAL))\n",
    "            if ori == ant:\n",
    "                antonym_intv.append(\n",
    "                    (ant_len, ant_len + len(ant.encode('utf-8')), REPLACE_ORIGINAL))\n",
    "                ori_ant_intv.append(\n",
    "                    (ori_len, ori_len + len(ori.encode('utf-8')), REPLACE_ORIGINAL))\n",
    "\n",
    "        ori_len = ori_len + len(ori.encode('utf-8')) + 1\n",
    "        # +1 to point the space before next token\n",
    "        syn_len = syn_len + len(syn.encode('utf-8')) + 1\n",
    "        ant_len = ant_len + len(ant.encode('utf-8')) + 1\n",
    "\n",
    "        synonym_sent.append(syn)\n",
    "        antonym_sent.append(ant)\n",
    "\n",
    "    doc._._synonym_sent = synonym_sent\n",
    "    # print(\"Synonym Sent: \\n\", synonym_sent)\n",
    "    synSentence = \"\"\n",
    "    for val in synonym_sent:\n",
    "        synSentence += val + \" \"\n",
    "    pairs.append(synSentence)\n",
    "    # print(\"Synonym Sentence Derived: \", synSentence)\n",
    "    doc._._synonym_intv = synonym_intv\n",
    "    # print(\"Synonym Intv: \\n\", synonym_intv)\n",
    "\n",
    "    doc._._ori_syn_intv = ori_syn_intv\n",
    "    # print(\"Ori Syn Intv:\\n \", ori_syn_intv)\n",
    "\n",
    "    doc._._antonym_sent = antonym_sent\n",
    "    # print(\"Anton Sent: \\n\", antonym_sent)\n",
    "    antSentence = \"\"\n",
    "    for val in antonym_sent:\n",
    "        antSentence += val + \" \"\n",
    "    pairs.append(antSentence)\n",
    "\n",
    "    doc._._antonym_intv = antonym_intv\n",
    "    # print(\"Anton Intv: \\n\", antonym_intv)\n",
    "\n",
    "    doc._._ori_ant_intv = ori_ant_intv\n",
    "    # print(\"Ori Ant Intv: \\n\", ori_ant_intv)\n",
    "\n",
    "    return doc\n",
    "\n",
    "\n",
    "def parsing_pipeline(given_sentence):\n",
    "    sentence_dict = dict()\n",
    "    parse_predict = predictor.predict(sentence=given_sentence)\n",
    "\n",
    "    sentence_dict[\"sentence\"] = given_sentence\n",
    "    sentence_dict['tokens'] = parse_predict['words']\n",
    "    sentence_dict['tags'] = parse_predict['pos']\n",
    "    \n",
    "    predicted_dependencies = parse_predict['predicted_dependencies']\n",
    "    predicted_heads = parse_predict['predicted_heads']\n",
    "    \n",
    "    sentence_dict['predicted_dependencies'] = parse_predict['predicted_dependencies']\n",
    "    sentence_dict['predicted_heads'] = parse_predict['predicted_heads']\n",
    "    sentence_dict['dependencies'] = []\n",
    "    \n",
    "    for idx, item in enumerate(predicted_dependencies):\n",
    "        dep_tag = item\n",
    "        frm = predicted_heads[idx]\n",
    "        to = idx + 1\n",
    "        sentence_dict['dependencies'].append([dep_tag, frm, to])\n",
    "        \n",
    "    sentence_dict[\"aspect_sentiment\"] = []\n",
    "    sentence_dict['from_to'] = [] #left and right offset of the target word \n",
    "\n",
    "    for index_sentence, tag in enumerate(sentence_dict[\"tags\"]):\n",
    "        if tag == \"NOUN\" or tag==\"PROPN\":\n",
    "            sentence_dict[\"aspect_sentiment\"].append((sentence_dict['tokens'][index_sentence]))\n",
    "            from_index = index_sentence\n",
    "            if from_index != 0:\n",
    "                to = sentence_dict['predicted_heads'][from_index - 1]\n",
    "                if sentence_dict['predicted_heads'][from_index] == to and sentence_dict['predicted_heads'][from_index + 1] !=to:\n",
    "                    to_index = sentence_dict['predicted_heads'][from_index + 1]\n",
    "                else:\n",
    "                    to_index = sentence_dict['predicted_heads'][from_index]\n",
    "\n",
    "\n",
    "            sentence_dict['from_to'].append((from_index, to_index))\n",
    "    \n",
    "    return sentence_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def createPairs(sentence):\n",
    "    '''\n",
    "        Input: Sentence\n",
    "        Output: Tuple/List of Two Synonym and Antonym Sentences Derived\n",
    "\n",
    "    '''\n",
    "    pairs = []\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        \"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"nlptown/bert-base-multilingual-uncased-sentiment\", config=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "    spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    doc = spacy_nlp(sentence)\n",
    "    replace_word(doc, pairs)\n",
    "\n",
    "    return pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your label namespace was 'pos'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonym replaced: {'sentence': 'This laptop computer is not mine ', 'tokens': ['This', 'laptop', 'computer', 'is', 'not', 'mine'], 'tags': ['DET', 'NOUN', 'NOUN', 'AUX', 'PART', 'ADJ'], 'predicted_dependencies': ['amod', 'amod', 'nsubj', 'cop', 'neg', 'root'], 'predicted_heads': [3, 3, 4, 6, 6, 0], 'dependencies': [['amod', 3, 1], ['amod', 3, 2], ['nsubj', 4, 3], ['cop', 6, 4], ['neg', 6, 5], ['root', 0, 6]], 'aspect_sentiment': ['laptop', 'computer'], 'from_to': [(1, 4), (2, 4)]}\n"
     ]
    }
   ],
   "source": [
    "sentence = \"This laptop is not mine\"\n",
    "syn_replace_sentence, ant_replace_sentence = createPairs(sentence)\n",
    "\n",
    "print(\"Synonym replaced: {}\".format(parsing_pipeline(syn_replace_sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antonym replaced: {'sentence': 'This heat is not mine ', 'tokens': ['This', 'heat', 'is', 'not', 'mine'], 'tags': ['DET', 'NOUN', 'AUX', 'PART', 'ADJ'], 'predicted_dependencies': ['quantmod', 'nsubj', 'cop', 'neg', 'root'], 'predicted_heads': [2, 3, 5, 5, 0], 'dependencies': [['quantmod', 2, 1], ['nsubj', 3, 2], ['cop', 5, 3], ['neg', 5, 4], ['root', 0, 5]], 'aspect_sentiment': ['heat'], 'from_to': [(1, 3)]}\n"
     ]
    }
   ],
   "source": [
    "print(\"Antonym replaced: {}\".format(parsing_pipeline(ant_replace_sentence)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
